{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwM6vbJfJ1Xv"
      },
      "source": [
        "# EE 508 HW 1 Part 2: Classification\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIsLSHBzJ1Xx"
      },
      "source": [
        "## Cross Validation, Bias-Variance trade-off, Overfitting\n",
        "\n",
        "In this section, we will demonstrate data splitting and the validation process in machine learning paradigms. We will use the Iris dataset from the `sklearn` library.\n",
        "\n",
        "Objective:\n",
        "- Train a Fully-Connected Network (FCN) for classification.  \n",
        "- Partition the data using three-fold cross-validation and report the training, validation, and testing accuracy.  \n",
        "- Train the model using cross-entropy loss and evaluate it with 0/1 loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iAwS23SvJ1Xx"
      },
      "outputs": [],
      "source": [
        "# import required libraries and dataset\n",
        "import numpy as np\n",
        "# load sklearn for ML functions\n",
        "from sklearn.datasets import load_iris\n",
        "# load torch dataaset for training NNs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(['ggplot'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyjgM1olJ1Xx"
      },
      "source": [
        "### **TODO 1**: Implement the cross validation function\n",
        "\n",
        "In this function:\n",
        "1.  Shuffle the dataset using `np.random.shuffle`.\n",
        "2.  Create partition indices using `np.linspace`.\n",
        "3.  Loop through `n_folds`:\n",
        "    *   Determine indices for `valid` set (from `partitions[i]` to `partitions[i+1]`).\n",
        "    *   Determine indices for `train` set (the rest).\n",
        "    *   Store the partitioned arrays `(x_train, y_train, x_valid, y_valid)` in `folds` list.\n",
        "4.  Return `folds`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hpkNYD-QJ1Xy"
      },
      "outputs": [],
      "source": [
        "# Skeleton for TODO 1\n",
        "def cross_validation(x: np.array, y: np.array, n_folds: int=3):\n",
        "    \"\"\"\n",
        "    Splitting the dataset to the given fold\n",
        "    Parameters:\n",
        "    - x: Feaures of the dataset, with shape (n_samples, n_features)\n",
        "    - y: Class label of the dataset, with shape (n_samples,)\n",
        "    - n_folds: the given number of partitions\n",
        "\n",
        "    Returns:\n",
        "    - folds (list): List of tuples (x_train, y_train, x_valid, y_valid)\n",
        "    \"\"\"\n",
        "    folds = []\n",
        "    # 1. Shuffle indices\n",
        "    # ... your code here ...\n",
        "    n_samples = y.shape[0]\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # 2. Divide indices into n_folds+1 partitions\n",
        "    # ... your code here ...\n",
        "    partitions = np.linspace(0, n_samples, n_folds+1)\n",
        "\n",
        "    # 3. Loop through folds\n",
        "    # ... your code here ...\n",
        "    for i in range(n_folds):\n",
        "      fold_start, fold_end = int(np.round(partitions[i])), int(np.round(partitions[i+1]))\n",
        "\n",
        "      valid_indices = indices[fold_start:fold_end]\n",
        "      train_indices = np.concatenate((indices[0:fold_start], indices[fold_end:n_samples]))\n",
        "\n",
        "      x_valid=x[valid_indices]\n",
        "      y_valid=y[valid_indices]\n",
        "\n",
        "      x_train=x[train_indices]\n",
        "      y_train=y[train_indices]\n",
        "\n",
        "      folds.append((x_train, y_train, x_valid, y_valid))\n",
        "\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J_GUA4I5J1Xy"
      },
      "outputs": [],
      "source": [
        "# Skeleton for TODO 1 (continued)\n",
        "# fixed the random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Load Iris dataset\n",
        "iris = load_iris()\n",
        "x, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into training and validation sets using cross_validation\n",
        "# three_folds = ...\n",
        "three_folds = cross_validation(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dW9J3-8J1Xy"
      },
      "source": [
        "### **TODO 2**: Build a Fully-Connect Networks with PyTorch\n",
        "\n",
        "Define a PyTorch model `FCN_model`:\n",
        "1.  In `__init__`, define 3 fully connected layers (`nn.Linear`):\n",
        "    *   Layer 1: Input size 4 -> `n_hidden` units.\n",
        "    *   Layer 2: `n_hidden` -> `n_hidden`.\n",
        "    *   Layer 3: `n_hidden` -> 3 (output classes).\n",
        "2.  In `forward`:\n",
        "    *   Pass input through Layer 1, apply ReLU.\n",
        "    *   Pass through Layer 2, apply ReLU.\n",
        "    *   Pass through Layer 3 (return logits, no activation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pmfLgF2mJ1Xy"
      },
      "outputs": [],
      "source": [
        "# Skeleton for TODO 2\n",
        "class FCN_model(nn.Module):\n",
        "    # take the argument for the number of hidden units\n",
        "    def __init__(self, n_hidden=32):\n",
        "        super(FCN_model, self).__init__()\n",
        "\n",
        "        # Activation Function\n",
        "        self.relu = nn.ReLU()\n",
        "        # 1. Define Layer 1: Linear(4, n_hidden)\n",
        "        # ... your code here ...\n",
        "        self.layer1 = nn.Linear(4, n_hidden)\n",
        "\n",
        "        # 2. Define Layer 2: Linear(n_hidden, n_hidden)\n",
        "        # ... your code here ...\n",
        "        self.layer2 = nn.Linear(n_hidden, n_hidden)\n",
        "\n",
        "        # 3. Define Output Layer: Linear(n_hidden, 3)\n",
        "        # ... your code here ...\n",
        "        self.output = nn.Linear(n_hidden, 3)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Apply Layer 1 + ReLU\n",
        "        # ... your code here ...\n",
        "        h1 = self.relu(self.layer1(x))\n",
        "\n",
        "\n",
        "        # 2. Apply Layer 2 + ReLU\n",
        "        # ... your code here ...\n",
        "        h2 = self.relu(self.layer2(h1))\n",
        "\n",
        "        # 3. Apply Output Layer\n",
        "        # ... your code here ...\n",
        "        return self.output(h2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nk8Lx08J1Xy"
      },
      "source": [
        "Set up the evaluation and training functions for the FCN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JUShUgGRJ1Xy"
      },
      "outputs": [],
      "source": [
        "def eval(model:nn.Module,\n",
        "         x:torch.tensor,\n",
        "         y:torch.tensor) -> float:\n",
        "    \"\"\"Evaluate the model: inference the model with 0/1 loss\n",
        "    We can define the output label is the maximum logit from the model\n",
        "\n",
        "    Parameters:\n",
        "    - model: the FCN model\n",
        "    - x: input features\n",
        "    - y: ground truth labels, dtype=long\n",
        "\n",
        "    Returns:\n",
        "    - loss: the average 0/1 loss value\n",
        "    \"\"\"\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = torch.argmax(model(x), dim=1)\n",
        "\n",
        "    loss = 0\n",
        "    for y_pred, y_gt in zip(preds, y):\n",
        "        if y_pred != y_gt:\n",
        "            loss += 1\n",
        "    print(f\"Averaging 0/1 loss: {loss/preds.shape[0]:.4f}\")\n",
        "    return loss/preds.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_ZW2FNzzJ1Xy"
      },
      "outputs": [],
      "source": [
        "def train(model:nn.Module,\n",
        "          x_train:torch.tensor,\n",
        "          y_train:torch.tensor,\n",
        "          x_valid:torch.tensor,\n",
        "          y_valid:torch.tensor,\n",
        "          epochs:int=300):\n",
        "    \"\"\"Training process\n",
        "    Parameters:\n",
        "    - model: the FCN model\n",
        "    - x_train, y_train: trainig features and labels (dtype=long)\n",
        "    - x_valid, y_valid: validation features and labels (dtype=long)\n",
        "    - epochs: number of the epoches for training\n",
        "    \"\"\"\n",
        "    # To simplify the process\n",
        "    # we do not take batches but use all the training samples\n",
        "    # set up the objective function and the optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        # Forward pass\n",
        "        outputs = model(x_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Cross Entropy Loss: {loss.item():.4f}\")\n",
        "            print(f\"[Train] \", end=\"\")\n",
        "            eval(model, x_train, y_train)\n",
        "            print(f\"[Valid] \", end=\"\")\n",
        "            eval(model, x_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2QZXrkKJ1Xz"
      },
      "source": [
        "### **TODO 3**: Conduct the training/validation process in each fold\n",
        "We will use three-fold validation.\n",
        "\n",
        "1.  Instantiate lists `train_losses` and `valid_losses`.\n",
        "2.  Loop through `three_folds`:\n",
        "    *   Instantiate `FCN_model(n_hidden=32)`.\n",
        "    *   Convert data arrays to `torch.Tensor` (and labels to `dtype=torch.long`).\n",
        "    *   Train the model for 500 epochs using `train()`.\n",
        "    *   Evaluate using `eval()` on training data and validation data, appending results to the lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVM6NR3EJ1Xz",
        "outputId": "03b19e2a-8e42-4a7c-8d5b-9b2ab39cbd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Training Fold 0 =====\n",
            "Epoch [100/300], Cross Entropy Loss: 0.7950\n",
            "[Train] Averaging 0/1 loss: 0.2700\n",
            "[Valid] Averaging 0/1 loss: 0.2800\n",
            "Epoch [200/300], Cross Entropy Loss: 0.5206\n",
            "[Train] Averaging 0/1 loss: 0.1400\n",
            "[Valid] Averaging 0/1 loss: 0.1000\n",
            "Epoch [300/300], Cross Entropy Loss: 0.3928\n",
            "[Train] Averaging 0/1 loss: 0.0600\n",
            "[Valid] Averaging 0/1 loss: 0.0400\n",
            "Averaging 0/1 loss: 0.0400\n",
            "Averaging 0/1 loss: 0.0600\n",
            "===== Training Fold 1 =====\n",
            "Epoch [100/300], Cross Entropy Loss: 0.8474\n",
            "[Train] Averaging 0/1 loss: 0.3300\n",
            "[Valid] Averaging 0/1 loss: 0.3400\n",
            "Epoch [200/300], Cross Entropy Loss: 0.5299\n",
            "[Train] Averaging 0/1 loss: 0.1900\n",
            "[Valid] Averaging 0/1 loss: 0.2000\n",
            "Epoch [300/300], Cross Entropy Loss: 0.3781\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.1000\n",
            "Averaging 0/1 loss: 0.1000\n",
            "Averaging 0/1 loss: 0.0400\n",
            "===== Training Fold 2 =====\n",
            "Epoch [100/300], Cross Entropy Loss: 0.6862\n",
            "[Train] Averaging 0/1 loss: 0.3100\n",
            "[Valid] Averaging 0/1 loss: 0.3400\n",
            "Epoch [200/300], Cross Entropy Loss: 0.4613\n",
            "[Train] Averaging 0/1 loss: 0.1500\n",
            "[Valid] Averaging 0/1 loss: 0.1400\n",
            "Epoch [300/300], Cross Entropy Loss: 0.3738\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Averaging 0/1 loss: 0.0600\n",
            "Averaging 0/1 loss: 0.0400\n"
          ]
        }
      ],
      "source": [
        "# Skeleton for TODO 3\n",
        "train_losses, valid_losses = [], []\n",
        "\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    print(f\"===== Training Fold {idx} =====\")\n",
        "    # 1. Instantiate FCN_model with 32 hidden units\n",
        "    # ... your code here ...\n",
        "    model = FCN_model()\n",
        "\n",
        "    # 2. Convert to Tensors\n",
        "    # ... your code here ...\n",
        "    x_train_t = torch.tensor(x_train, dtype = torch.float32)\n",
        "    y_train_t = torch.tensor(y_train)\n",
        "    x_valid_t = torch.tensor(x_valid, dtype = torch.float32)\n",
        "    y_valid_t = torch.tensor(y_valid)\n",
        "\n",
        "\n",
        "\n",
        "    # 3. Train\n",
        "    # ... your code here ...\n",
        "    train(model, x_train_t, y_train_t, x_valid_t, y_valid_t)\n",
        "\n",
        "    # 4. Evaluate and store results\n",
        "    # ... your code here ...\n",
        "    valid_losses.append(eval(model, x_valid_t, y_valid_t))\n",
        "    train_losses.append(eval(model, x_train_t, y_train_t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM9ipdq1J1Xz",
        "outputId": "6f8f640f-8282-47db-de13-52eb16c32a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.06,            0.04\n",
            "    1,          0.04,            0.10\n",
            "    2,          0.04,            0.06\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_losses, valid_losses)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFx4-eMAJ1Xz"
      },
      "source": [
        "### **TODO 4**: Check over-fitting with complex model\n",
        "Repeat the procedure from TODO 3, but this time use a more complex model:\n",
        "*   Set `n_hidden` to 2048.\n",
        "*   Train for 500 epochs.\n",
        "*   Store results in `train_overfit` and `valid_overfit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VesQdlvaJ1Xz",
        "outputId": "65eca8d4-b5cf-49b9-a064-637832ef2fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Training Fold 0 =====\n",
            "Epoch [100/300], Cross Entropy Loss: 0.3259\n",
            "[Train] Averaging 0/1 loss: 0.2100\n",
            "[Valid] Averaging 0/1 loss: 0.1800\n",
            "Epoch [200/300], Cross Entropy Loss: 0.2201\n",
            "[Train] Averaging 0/1 loss: 0.1000\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Epoch [300/300], Cross Entropy Loss: 0.1297\n",
            "[Train] Averaging 0/1 loss: 0.0500\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0500\n",
            "===== Training Fold 1 =====\n",
            "Epoch [100/300], Cross Entropy Loss: 0.3055\n",
            "[Train] Averaging 0/1 loss: 0.1400\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Epoch [200/300], Cross Entropy Loss: 0.1588\n",
            "[Train] Averaging 0/1 loss: 0.0700\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [300/300], Cross Entropy Loss: 0.0762\n",
            "[Train] Averaging 0/1 loss: 0.0000\n",
            "[Valid] Averaging 0/1 loss: 0.0400\n",
            "Averaging 0/1 loss: 0.0400\n",
            "Averaging 0/1 loss: 0.0000\n",
            "===== Training Fold 2 =====\n",
            "Epoch [100/300], Cross Entropy Loss: 0.3189\n",
            "[Train] Averaging 0/1 loss: 0.1600\n",
            "[Valid] Averaging 0/1 loss: 0.2000\n",
            "Epoch [200/300], Cross Entropy Loss: 0.2348\n",
            "[Train] Averaging 0/1 loss: 0.0900\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Epoch [300/300], Cross Entropy Loss: 0.1641\n",
            "[Train] Averaging 0/1 loss: 0.0800\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Averaging 0/1 loss: 0.0600\n",
            "Averaging 0/1 loss: 0.0800\n"
          ]
        }
      ],
      "source": [
        "# Skeleton for TODO 4\n",
        "train_overfit, valid_overfit = [], []\n",
        "\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    print(f\"===== Training Fold {idx} =====\")\n",
        "    # 1. Instantiate FCN_model with 2048 hidden units\n",
        "    # ... your code here ...\n",
        "    model = FCN_model(2048)\n",
        "\n",
        "    # 2. Convert to Tensors\n",
        "    # ... your code here ...\n",
        "    x_train_t = torch.tensor(x_train, dtype = torch.float32)\n",
        "    y_train_t = torch.tensor(y_train)\n",
        "    x_valid_t = torch.tensor(x_valid, dtype = torch.float32)\n",
        "    y_valid_t = torch.tensor(y_valid)\n",
        "\n",
        "    # 3. Train\n",
        "    # ... your code here ...\n",
        "    train(model, x_train_t, y_train_t, x_valid_t, y_valid_t)\n",
        "\n",
        "    # 4. Evaluate and store results\n",
        "    # ... your code here ...\n",
        "    valid_overfit.append(eval(model, x_valid_t, y_valid_t))\n",
        "    train_overfit.append(eval(model, x_train_t, y_train_t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEeSpX-FJ1Xz",
        "outputId": "2f8c81f2-01eb-49b4-b79e-2d50ae3a6a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.05,            0.02\n",
            "    1,          0.00,            0.04\n",
            "    2,          0.08,            0.06\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_overfit, valid_overfit)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azdnnf07J1X0"
      },
      "source": [
        "### **TODO 5**: Compare the FCN with statistical ML models\n",
        "Use `sklearn.naive_bayes.GaussianNB`:\n",
        "1.  Loop through `three_folds`.\n",
        "2.  Instantiate `GaussianNB`.\n",
        "3.  Fit the model on training data.\n",
        "4.  Calculate error (1 - accuracy) for training and validation sets using `model.score()`.\n",
        "5.  Store errors in `train_nb` and `valid_nb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ysf8wdL0J1X0"
      },
      "outputs": [],
      "source": [
        "# Skeleton for TODO 5\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "train_nb, valid_nb = [], []\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    # 1. Instantiate GaussianNB\n",
        "    # ... your code here ...\n",
        "    model = GaussianNB()\n",
        "\n",
        "    # 2. Fit model\n",
        "    # ... your code here ...\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # 3. Calculate error (1 - score) for train and valid\n",
        "    # ... your code here ...\n",
        "    train_nb.append(1-model.score(x_train, y_train))\n",
        "    valid_nb.append(1-model.score(x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzf1b-BSJ1X0",
        "outputId": "94276c2e-5680-4883-f67a-7e31c71b0b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.05,            0.04\n",
            "    1,          0.02,            0.06\n",
            "    2,          0.04,            0.04\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_nb, valid_nb)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJyX98NyJ1X0"
      },
      "source": [
        "### **TODO 6**:\n",
        "Answer the following questions in the next cell.  \n",
        "1. What is the the bias-variance trade-off in machine learning?\n",
        "2. How to reduce overfitting and underfitting?\n",
        "3. How do the training and inference processes differ between the Naive Bayes model and a fully connected neural network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCG8iNKkJ1X0"
      },
      "source": [
        "Your answer:\n",
        "\n",
        "1. The bias variance tradeoff is a mathematical tool to describe the relationship between model complexity/variance across training datasets and the inherent distance/bias a model family has to the true underlying model. As we tend to increase the range of models we can fit, this decreases bias but increases the variance we get from model training. When we restrict the model family, we decrease this variance but increase the bias the converged model would have.\n",
        "\n",
        "2. We can reduce overfitting by decreasing the model complexity by either restricting the amount of parameters we have or regularizing the model. We can reduce underfitting by making our model family more complex.\n",
        "\n",
        "3. Naive bayes learns through a frequency table and a strong assumption that there is no cross conditional dependence of any of the features of x and y (ie p(X|y) = p(x1|y)*p(x2|y)...). Neural networks don't assume a structure/bias like this and instead learn in a gradient approached manner to minimize a function in relation to a defined loss like cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoLiMfxXhc6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "caption",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}